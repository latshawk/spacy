{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSPECTING THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_d = '''Under the federal securities laws, any offer or sale of a security must either be registered with the SEC or meet an exemption. \n",
    "Regulation D under the Securities Act provides a number of exemptions from the registration requirements, allowing some companies \n",
    "to offer and sell their securities without having to register the offering with the SEC. For more information about these exemptions, \n",
    "see our Fast Answers on Rules 504 and 506 of Regulation D.Companies that comply with the requirements of Regulation D do not have to \n",
    "register their offering of securities with the SEC, but they must file what’s known as a \"Form D\" electronically with the SEC after \n",
    "they first sell their securities. Form D is a brief notice that includes the names and addresses of the company’s promoters, executive \n",
    "officers and directors, and some details about the offering, but contains little other information about the company. You can access the \n",
    "SEC’s EDGAR database to determine whether the company has filed a Form D. Even if a company takes advantage of an exemption from registration, \n",
    "a company should take care to provide sufficient information to investors to avoid violating the antifraud provisions of the securities laws. \n",
    "This means that any information a company provides to investors must be free from false or misleading statements. Similarly, a company should not \n",
    "exclude any information if the omission makes what is provided to investors false or misleading. You should always check with your state securities \n",
    "regulator to see if they have more information about the company and the people behind it. Be sure to ask whether your state regulator has received \n",
    "notice of the offering or, in the case of a Rule 504 offering, cleared the offering for sale in your state. You can get the address and telephone number \n",
    "for your state securities regulator by calling the North American Securities Administrators Association at (202) 737-0900 or by visiting its website.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x000001E7D476F438>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x000001E7D5D84108>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x000001E7D5D84168>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "def length_component(doc):\n",
    "    doc_len = len(doc)\n",
    "    print(\"doc is {} tokens long\".format(doc_len))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#add the component to the pipeline\n",
    "nlp.add_pipe(length_component, first=True) #before, #after, #last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc is 5 tokens long\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animal_component(doc):\n",
    "    #apply matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    #create SPAN for ANIMAL type in doc\n",
    "    spans = [Span(doc, start, end, label='ANIMALS') for match_id, start, end in matches]\n",
    "    \n",
    "    #overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'animal_component' already exists in pipeline. Existing names: ['length_component', 'tagger', 'parser', 'ner', 'animal_component']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1759fed67a99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#add component to the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manimal_component\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I have a cat and a Golden Retriever\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E007] 'animal_component' already exists in pipeline. Existing names: ['length_component', 'tagger', 'parser', 'ner', 'animal_component']"
     ]
    }
   ],
   "source": [
    "#add component to the pipeline\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETTING EXTENSION ATTRIBUTES TO DOCS | SPANS | TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register the token extension attribute \"is_country\" with the default value to FALSE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "Token.set_extension(\"is_country\", default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "#process text and set \"is_country\" attribute equal to True\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "#set the value \"Spain\" to True\n",
    "doc[3]._.is_country = True\n",
    "print([(token.text, token._.is_country) for token in doc] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define getter function taking token and returning the reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed:  llA\n",
      "reversed:  snoitazilareneg\n",
      "reversed:  era\n",
      "reversed:  eslaf\n",
      "reversed:  ,\n",
      "reversed:  gnidulcni\n",
      "reversed:  siht\n",
      "reversed:  eno\n",
      "reversed:  .\n"
     ]
    }
   ],
   "source": [
    "#register token property extension with the getter function\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "#process text and print reversed\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed: ', token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "#define the getter function\n",
    "def get_has_number(doc):\n",
    "    #return if any of the tokens in the doc return True\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "#register doc property extension \"has_number\" with the getter has number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "#processing text and check custom number \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello World</strong>\n"
     ]
    }
   ],
   "source": [
    "#set a HTML tag\n",
    "def to_html(span, tag):\n",
    "    #wrap the span text in HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "#register Span extension with method HTML (instead of getter)\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "#process text and check for tags\n",
    "doc = nlp(\"Hello World, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Components with extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'countries_component']\n"
     ]
    }
   ],
   "source": [
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\")\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Register capital and getter that looks up the span text in country capitals\n",
    "Span.set_extension('capital', getter=lambda span: capitals.get(span.text), force=True)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROCESSING STREAMS !!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BAD EXAMPLE \n",
    "    #docs = [nlp(text) for text in LOTS_O_TEXT]\n",
    "    \n",
    "#GOOD EXAMPLE\n",
    "    #docs = list(nlp.pipe(LOTS_O_TEXTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() () () () () () ()\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[McDonalds is my favorite restaurant.,\n",
       " Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..,\n",
       " People really still eat McDonalds :(,\n",
       " The McDonalds in Spain has chicken wings. My heart is so happy ,\n",
       " @McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P,\n",
       " please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D,\n",
       " This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[David Bowie, Angela Merkel, Lady Gaga]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-824e49a08d8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Set the doc._.book and doc._.author attributes from the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'book'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DATA' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the Doc class and register the extensions 'author' and 'book'\n",
    "from spacy.tokens import Doc\n",
    "Doc.set_extension('book', default=None)\n",
    "Doc.set_extension('author', default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(doc):\n",
    "    #get spacys default stopword list(312 words)\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    from string import punctuation\n",
    "    punct = set(punctuation)\n",
    "    ALL_STOPS = spacy_stopwords.union(punct)\n",
    "    #add custom stopword\n",
    "    #ALL_STOPS.add(\"\")\n",
    "    for stopword in ALL_STOPS:\n",
    "        lexeme = nlp.vocab[stopword]\n",
    "        lexeme.is_stop = True\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            token.text\n",
    "        \n",
    "        \n",
    "    \n",
    "   \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'stop_words', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(stop_words, after='tagger')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(reg_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords.add(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"message\"].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def stop_words(doc):\n",
    "    #get spacys default stopword list(312 words)\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "   \n",
    "    punct = set(punctuation)\n",
    "    ALL_STOPS = spacy_stopwords.union(punct)\n",
    "    for stopword in ALL_STOPS:\n",
    "        lexeme = nlp.vocab[stopword]\n",
    "        lexeme.is_stop = True\n",
    "    #for token in doc:\n",
    "     #   if not token.is_stop:\n",
    "      #      token.text\n",
    "        \n",
    "        \n",
    "    \n",
    "   \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Under the federal securities laws, any offer or sale of a security must either be registered with the SEC or meet an exemption. \n",
       "Regulation D under the Securities Act provides a number of exemptions from the registration requirements, allowing some companies \n",
       "to offer and sell their securities without having to register the offering with the SEC. For more information about these exemptions, \n",
       "see our Fast Answers on Rules 504 and 506 of Regulation D.Companies that comply with the requirements of Regulation D do not have to \n",
       "register their offering of securities with the SEC, but they must file what’s known as a \"Form D\" electronically with the SEC after \n",
       "they first sell their securities. Form D is a brief notice that includes the names and addresses of the company’s promoters, executive \n",
       "officers and directors, and some details about the offering, but contains little other information about the company. You can access the \n",
       "SEC’s EDGAR database to determine whether the company has filed a Form D. Even if a company takes advantage of an exemption from registration, \n",
       "a company should take care to provide sufficient information to investors to avoid violating the antifraud provisions of the securities laws. \n",
       "This means that any information a company provides to investors must be free from false or misleading statements. Similarly, a company should not \n",
       "exclude any information if the omission makes what is provided to investors false or misleading. You should always check with your state securities \n",
       "regulator to see if they have more information about the company and the people behind it. Be sure to ask whether your state regulator has received \n",
       "notice of the offering or, in the case of a Rule 504 offering, cleared the offering for sale in your state. You can get the address and telephone number \n",
       "for your state securities regulator by calling the North American Securities Administrators Association at (202) 737-0900 or by visiting its website."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
